{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP4318 & 5318 - Machine Learning and Data Mining: Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due: Sunday Week 7 - Sep 15th, 2024 11:59PM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Install and import necessary libraries\n",
    "# General\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Preprocessing/processing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler # (?) might remove\n",
    "from sklearn.decomposition import PCA\n",
    "from skimage.feature import hog\n",
    "\n",
    "# Models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "# Evaluation & Finetuning\n",
    "from sklearn.model_selection import GridSearchCV, KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score, mean_absolute_error, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define any necessary utility or helper functions (e.g., for plotting, optimization, etc.) if applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define helper function (e.g. plotting) if applicable\n",
    "\n",
    "def get_hog_features(row, image_shape=(28, 28)):\n",
    "    # Step 1: Reshape the flattened row (1D array) back to 2D\n",
    "    reshaped_image = row.values.reshape(image_shape)\n",
    "    \n",
    "    # Step 2: Apply HOG feature extraction\n",
    "    hog_features = hog(reshaped_image, \n",
    "                       orientations=16, \n",
    "                       pixels_per_cell=(3, 3),\n",
    "                       cells_per_block=(2, 2), \n",
    "                       block_norm='L2-Hys', \n",
    "                       visualize=False)\n",
    "    return hog_features\n",
    "\n",
    "#set seed\n",
    "seed = 2307\n",
    "\n",
    "# Label mapping for visualization\n",
    "label_mapping = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement at least ONE preprocessing technique on the dataset before model training. Possible methods include **Normalization**, **Dimensionality Reduction**, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement Preprocessing Techniques\n",
    "\n",
    "# Read file\n",
    "pd.options.mode.chained_assignment = None\n",
    "df = pd.read_csv('data/train.csv')\n",
    "df_test = pd.read_csv('data/test1.csv')\n",
    "df_test_kaggle = pd.read_csv('data/test2.csv')\n",
    "\n",
    "# Seperate features and labels\n",
    "X = df.loc[:, \"v1\":\"v784\"]\n",
    "Y = df.loc[:, \"label\"]\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.3, stratify=Y, random_state=seed)\n",
    "\n",
    "X_test = df_test.loc[:, \"v1\":\"v784\"]\n",
    "Y_test = df_test.loc[:, \"label\"]\n",
    "X_kaggle = df_test_kaggle.loc[:, \"v1\":\"v784\"]\n",
    "\n",
    "# Normalization\n",
    "X_train = X_train/255\n",
    "X_val = X_val/255\n",
    "X_test = X_test/255\n",
    "X_kaggle = X_kaggle/255\n",
    "\n",
    "# Histogram of Oriented Gradients (HOG)\n",
    "X_train_hog = pd.DataFrame(X_train.apply(get_hog_features, axis=1).tolist())\n",
    "X_val_hog = pd.DataFrame(X_val.apply(get_hog_features, axis=1).tolist())\n",
    "X_test_hog = pd.DataFrame(X_test.apply(get_hog_features, axis=1).tolist())\n",
    "X_kaggle_hog = pd.DataFrame(X_kaggle.apply(get_hog_features, axis=1).tolist())\n",
    "\n",
    "# Dimensionality reduction using PCA\n",
    "#pca = PCA(n_components=0.99, random_state=seed)\n",
    "#X_train_hog_pca = pca.fit_transform(X_train_hog_df)\n",
    "#X_test_hog_pca = pca.transform(X_test_hog_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dimensionality reduction using PCA\n",
    "pca = PCA(random_state=seed)\n",
    "pca.fit(X_train_hog)\n",
    "\n",
    "# Plot the cumulative variance ratio (cvr) vs number of PCs\n",
    "evr = pca.explained_variance_ratio_ \n",
    "cvr = np.cumsum(evr)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(cvr, marker='o', linestyle='--')\n",
    "plt.axhline(y=0.95, color='r', linestyle='--')\n",
    "plt.title('Cumulative Explained Variance vs. Number of Components')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Find the number of components that maintain 95% of the variance\n",
    "n_components_95 = np.argmax(cvr >= 0.95) + 1\n",
    "print(f'Number of components to maintain 95% variance: {n_components_95}')\n",
    "\n",
    "# Return new training and test data \n",
    "pca = PCA(n_components = n_components_95)\n",
    "X_train_pca, X_val_pca, X_test_pca, X_kaggle_pca = pca.fit_transform(X_train_hog), pca.fit_transform(X_val_hog), pca.transform(X_test_hog), pca.transform(X_kaggle_hog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Model 1 - KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement model 1 \n",
    "base_knn = KNeighborsClassifier()\n",
    "\n",
    "# Fit kNN\n",
    "start_time = time.time()\n",
    "base_knn.fit(X_train_pca, Y_train)\n",
    "end_time = time.time()\n",
    "print(f\"Time to fit kNN: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Predict and evaluate on validation data\n",
    "Y_pred_knn = base_knn.predict(X_val_pca)\n",
    "accuracy = accuracy_score(Y_val, Y_pred_knn)\n",
    "print(f\"kNN Accuracy: {accuracy:.3f}\")\n",
    "\n",
    "#Time to fit kNN: 0.32 seconds\n",
    "#Time to predict with kNN: 3.66 seconds\n",
    "#kNN Accuracy:, 0.849"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper-parameters Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_neighbors': np.arange(1, 21),  # Number of neighbors to try (from 1 to 20)\n",
    "    'weights': ['uniform', 'distance'],  # Weight function used in prediction\n",
    "    'p': [1, 2]  # Power parameter for the Minkowski distance (1 is Manhattan, 2 is Euclidean)\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "knn_cv = GridSearchCV(\n",
    "    knn, \n",
    "    param_grid=param_grid, \n",
    "    cv=5,\n",
    "    scoring='accuracy', \n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Fit knn model\n",
    "start_time = time.time()\n",
    "knn_cv.fit(X_train_pca, Y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "# Print the best parameters and the best score\n",
    "print(f\"Best parameters: {knn_cv.best_params_}\")\n",
    "print(f\"Best cross-validation accuracy: {knn_cv.best_score_:.2f}\")\n",
    "\n",
    "total_fits = len(knn_cv.cv_results_['mean_test_score']) * knn_cv.cv\n",
    "knn_training_time = (start_time-end_time)/total_fits\n",
    "\n",
    "# Predict on test data\n",
    "best_knn = knn_cv.best_estimator_\n",
    "Y_pred_knn = best_knn.predict(X_val_pca)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(Y_val, Y_pred_knn)\n",
    "print(f\"kNN Accuracy: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Model 2 - SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement model 2\n",
    "base_svm = SVC(random_state = seed)\n",
    "\n",
    "# Fit SVM model\n",
    "start_time = time.time()\n",
    "bsae_svm.fit(X_train_pca, Y_train)\n",
    "end_time = time.time()\n",
    "print(f\"Time to fit SVM: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Predict and evaluate on validation data\n",
    "Y_pred_svm = base_svm.predict(X_val_pca)\n",
    "accuracy = accuracy_score(Y_val, Y_pred_svm)\n",
    "print(f\"SVM Accuracy: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper-parameters Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetune hyperparameters using gridsearch and k-fold cross validation\n",
    "start_time = time.time()\n",
    "\n",
    "svm = SVC(kernel='rbf',\n",
    "         random_state = seed)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100, 1000],\n",
    "    'gamma': [0.01, 0.1, 1, 10]   # Regularisation parameter for SVM\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "svm_cv = GridSearchCV(\n",
    "    svm, \n",
    "    param_grid=param_grid, \n",
    "    cv=5,\n",
    "    scoring='accuracy', \n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Fit knn model\n",
    "svm_cv.fit(X_train_pca, Y_train)\n",
    "\n",
    "# Print the best parameters and the best score\n",
    "print(f\"Best parameters: {svm_cv.best_params_}\")\n",
    "print(f\"Best cross-validation accuracy: {svm_cv.best_score_:.2f}\")\n",
    "\n",
    "# Predict on test data\n",
    "best_svm = svm_cv.best_estimator_\n",
    "Y_pred_svm = best_svm.predict(X_test_pca)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(Y_test, Y_pred_svm)\n",
    "print(f\"SVM Accuracy: {accuracy:.3f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Cell completion time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Model 3 - Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement model 3\n",
    "# Decision Tree\n",
    "base_dt = DecisionTreeClassifier(criterion='entropy', \n",
    "                             splitter='best', \n",
    "                             max_depth=None, \n",
    "                             #min_samples_split=2, \n",
    "                             #min_samples_leaf=1, \n",
    "                             min_weight_fraction_leaf=0.0, \n",
    "                             max_features=None, \n",
    "                             max_leaf_nodes=None, \n",
    "                             min_impurity_decrease=0, \n",
    "                             class_weight=None, \n",
    "                             #ccp_alpha=0.001,\n",
    "                             random_state=seed)\n",
    "\n",
    "# Fit model\n",
    "start_time = time.time()\n",
    "base_dt.fit(X_train_pca, Y_train)\n",
    "end_time = time.time()\n",
    "print(f\"Time to fit Decision Tree: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Predict and evaluate on validation data\n",
    "Y_pred_dt = base_dt.predict(X_val_pca)\n",
    "accuracy = accuracy_score(Y_val, Y_pred_dt)\n",
    "print(f\"Decision Tree Validation Accuracy: {accuracy:.3f}\")\n",
    "#Y_pred_train = clf_dt.predict(X_train_pca)\n",
    "#print(\"Train Accuracy:\", accuracy_score(Y_train, Y_pred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper-parameters Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetune hyperparameters using gridsearch and k-fold cross validation\n",
    "start_time = time.time()\n",
    "\n",
    "dt = DecisionTreeClassifier(splitter='best',\n",
    "                            min_impurity_decrease=0, \n",
    "                            class_weight=None, \n",
    "                            #ccp_alpha=0.001,\n",
    "                            random_state=seed)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 5, 10, 20],\n",
    "    'max_features': [None, 'sqrt', 'log2'],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 5, 10],\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "dt_cv = GridSearchCV(\n",
    "    dt, \n",
    "    param_grid=param_grid, \n",
    "    cv=5,\n",
    "    scoring='accuracy', \n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Fit knn model\n",
    "dt_cv.fit(X_train_pca, Y_train)\n",
    "\n",
    "# Print the best parameters and the best score\n",
    "print(f\"Best parameters: {dt_cv.best_params_}\")\n",
    "print(f\"Best cross-validation accuracy: {dt_cv.best_score_:.2f}\")\n",
    "\n",
    "# Predict on test data\n",
    "best_dt = dt_cv.best_estimator_\n",
    "Y_pred_dt = best_dt.predict(X_test_pca)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(Y_test, Y_pred_svm)\n",
    "print(f\"SVM Accuracy: {accuracy:.3f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Cell completion time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Model 4 - XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement model 4\n",
    "# XGBoost\n",
    "base_xgb = xgb.XGBClassifier(objective=\"multi:softmax\",\n",
    "                          tree_method=\"approx\",\n",
    "                          eval_metric=\"mlogloss\",\n",
    "                          random_state = seed)\n",
    "\n",
    "start_time = time.time()\n",
    "base_xgb.fit(X_train, Y_train)\n",
    "end_time = time.time()\n",
    "print(f\"Time to fit XGBoost: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "Y_pred_xgb = base_xgb.predict(X_val_pca)\n",
    "accuracy = accuracy_score(Y_val, Y_pred_xgb)\n",
    "print(f\"XGBoost Validation Accuracy: {accuracy:.3f}\")\n",
    "#Y_pred_train = clf_xgb.predict(X_train)\n",
    "#print(\"Train Accuracy:\", accuracy_score(Y_train, Y_pred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper-parameters Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetune hyperparameters using gridsearch and k-fold cross validation\n",
    "start_time = time.time()\n",
    "\n",
    "xgb = xgb.XGBClassifier(objective=\"multi:softmax\",\n",
    "                       tree_method=\"approx\",\n",
    "                       eval_metric=\"mlogloss\",\n",
    "                       learning_rate=0.3,\n",
    "                       random_state = seed)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'max_depth':[3, 4, 5], \n",
    "    'n_estimators':[100, 150, 200],\n",
    "    'reg_lambda': [0, 100]\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "xgb_cv = GridSearchCV(\n",
    "    xgb, \n",
    "    param_grid=param_grid, \n",
    "    cv=5,\n",
    "    scoring='accuracy', \n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Fit knn model\n",
    "xgb_cv.fit(X_train_pca, Y_train)\n",
    "\n",
    "# Print the best parameters and the best score\n",
    "print(f\"Best parameters: {xgb_cv.best_params_}\")\n",
    "print(f\"Best cross-validation accuracy: {xgb_cv.best_score_:.2f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Cell completion time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the best version of each model using appropriate classification performance metrics on the validation set and test on `test1.csv`. Ensure that the results are visualized using high-quality plots, figures, or tables to clearly demonstrate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate each model\n",
    "\n",
    "# Predict on test data\n",
    "best_knn = knn_cv.best_estimator_\n",
    "best_svm = svm_cv.best_estimator_\n",
    "best_dt = dt_cv.best_estimator_\n",
    "best_xgb = xgb_cv.best_estimator_\n",
    "\n",
    "models = [best_knn, best_svm, best_dt, best_xgb]\n",
    "model_names = ['KNN', 'SVM', 'DT', 'XGBoost']\n",
    "val_accuracy = []\n",
    "test_accuracy = []\n",
    "inference_time = []\n",
    "\n",
    "# Get accuracy\n",
    "for model in models:\n",
    "    Y_val_pred = model.predict(X_val_pca)\n",
    "    val_accuracy.append(accuracy_score(Y_val, Y_val_pred))\n",
    "\n",
    "    start_time = time.time()\n",
    "    Y_test_pred = model.predict(X_test_pca)\n",
    "    test_accuracy.append(accuracy_score(Y_test, Y_test_pred))\n",
    "    end_time = time.time()\n",
    "    \n",
    "    inference_time.append((end_time-start_time)/len(Y_test))\n",
    "\n",
    "bar_width = 0.35\n",
    "x_pos = np.arange(len(models))\n",
    "\n",
    "# Plot Model Accuracy\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(x_pos - bar_width/2, val_accuracy, width=bar_width, label='Validation Accuracy', color='skyblue')\n",
    "plt.bar(x_pos + bar_width/2, test_accuracy, width=bar_width, label='Test Accuracy', color='lightgreen')\n",
    "plt.title('Model Accuracy on Validation and Test Data', fontsize=16)\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(x_pos, model_names)\n",
    "plt.show()\n",
    "\n",
    "# Plot Inference Time\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(model_names, inference_time, color='skyblue')\n",
    "plt.title('Model Mean Inference time')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Time (s)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(Y_test, Y_pred_svm)\n",
    "print(f\"SVM Accuracy: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare all classifiers with their optimized hyper-parameters, focusing on criteria such as classification performance, training time, and inference time. Visualization of these comparisons is required; use high-quality plots, figures, or tables to facilitate a clear understanding of the differences and strengths of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare performance of all models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. The Best Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclude the best classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train and test the classifier which has the best performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Predict on Kaggle Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the testing data for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test2.csv includes 5000 samples used for label prediction. Test samples do not have labels.\n",
    "data_test_df = pd.read_csv('./data/test2.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the your best classifier to make predictions for the test data. The predictions should be stored in a vector named `output`, with a length of 5000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use your best classifier to make predictions on unseen data. The output of this code must be a vector named 'output' of length 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save your prediction vector as a `test_output.csv` file, which contains two columns: `id` and `label`. Please refer to the `example_output.csv` for the structure of this output file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.DataFrame(output, columns = ['label'])\n",
    "output_df.to_csv('./test_output.csv', sep=\",\", float_format='%d',index_label=\"id\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
